# ğŸš€ BeyondChats â€“ AI-Powered Blog Automation System

An end-to-end full-stack automation system that scrapes blog articles, enriches them using competitor analysis and Large Language Models (LLMs), and exposes clean REST APIs for content management.

Built as part of the **BeyondChats Full Stack Developer Intern Assignment**.

---

## ğŸ“Œ Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [System Architecture](#system-architecture)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Phase-wise Implementation](#phase-wise-implementation)
- [Setup & Installation](#setup--installation)
- [API Endpoints](#api-endpoints)
- [Automation Workflow](#automation-workflow-phase-2)
- [Error Handling & Design Decisions](#error-handling--design-decisions)
- [Future Improvements](#future-improvements)
- [Author](#author)

---

## ğŸ“– Overview

This project automates the process of improving blog articles using AI.

### What it does:
- Scrapes the **5 oldest blog articles** from BeyondChats
- Stores them in MongoDB
- Exposes CRUD APIs to manage articles
- Automatically:
  - Searches Google for competitor articles
  - Scrapes competitor content
  - Uses **Google Gemini LLM** to improve the original article
  - Updates the article via REST API
  - Stores reference URLs

The system is designed to be **fault-tolerant, modular, and scalable**.

---

## âœ¨ Key Features

- âœ… Web scraping with correct chronological handling  
- âœ… MongoDB persistence with clean schema design  
- âœ… RESTful CRUD APIs (tested with Postman)  
- âœ… Google Search integration using Serper.dev  
- âœ… External article scraping with fallback logic  
- âœ… AI-powered rewriting using Google Gemini (**gemini-2.5-flash**)  
- âœ… End-to-end automation pipeline  
- âœ… ES Modulesâ€“based modern Node.js architecture  
- âœ… Clean Git history with logical commits  

---

## ğŸ§  System Architecture

```mermaid
flowchart LR
    A[BeyondChats Blog Site] --> B[Scraping Phase 1]
    B --> C[MongoDB Articles]
    C --> D[Express REST APIs]
    D --> E[CRUD Routes]

    C --> F[Automation Phase 2]
    F --> G[Google Search API]
    G --> H[Competitor Blog Pages]
    H --> I[Gemini LLM Rewrite]
    I --> J[Update Article via PUT API]
```
## ğŸ›  Tech Stack

### Backend
Node.js
Express.js
MongoDB Atlas
Mongoose
Axios
Cheerio

### Automation & AI
Serper.dev (Google Search API)
Google Gemini (gemini-2.5-flash)
Prompt Engineering for LLMs

### Tooling
Postman (API testing)
Nodemon
Git & GitHub

## ğŸ“‚ Project Structure

src/
â”œâ”€â”€ app.js                 # Express app (ES Modules)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Article.js         # MongoDB schema
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ articleController.js
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ articleRoutes.js
â”œâ”€â”€ scrapers/
â”‚   â””â”€â”€ scrapeBlogs.js     # Phase 1 scraper
â”œâ”€â”€ automation/
â”‚   â”œâ”€â”€ fetchArticles.js
â”‚   â”œâ”€â”€ searchGoogle.js
â”‚   â”œâ”€â”€ scrapeExternal.js
â”‚   â”œâ”€â”€ llmRewrite.js
â”‚   â”œâ”€â”€ updateArticle.js
â”‚   â””â”€â”€ runAutomation.js
â””â”€â”€ .env (ignored)


## ğŸ§© Phase-wise Implementation
ğŸ”¹ Phase 1 â€“ Data Collection
Scraped 5 oldest blog articles

Used backward pagination for correct chronology

Stored articles in MongoDB

ğŸ”¹ Phase 1.5 â€“ Backend APIs
Implemented full CRUD APIs

Tested all endpoints using Postman

ğŸ”¹ Phase 2 â€“ Automation & AI
Fetch articles via API

Search competitors using Google Search API

Scrape competitor content

Rewrite articles using Gemini LLM

Update articles via PUT API

Store reference URLs

âš™ï¸ Setup & Installation
1ï¸âƒ£ Clone Repository
bash
Copy code
git clone <repo-url>
cd beyondchats-backend
2ï¸âƒ£ Install Dependencies
bash
Copy code
npm install
3ï¸âƒ£ Environment Variables (.env)
bash
Copy code
PORT=5000
MONGO_URI=your_mongodb_uri
SERPER_API_KEY=your_serper_key
GEMINI_API_KEY=your_gemini_key
4ï¸âƒ£ Run Backend
bash
Copy code
npm run dev
5ï¸âƒ£ Run Automation
bash
Copy code
node src/automation/runAutomation.js
ğŸ”Œ API Endpoints
Method	Endpoint	Description
GET	/api/articles	Get all articles
GET	/api/articles/:id	Get article by ID
POST	/api/articles	Create article
PUT	/api/articles/:id	Update article
DELETE	/api/articles/:id	Delete article

ğŸ” Automation Workflow (Phase 2)
For each non-updated article:

Fetch article via API

Search title on Google

Scrape 2 competitor blogs

Rewrite content using Gemini

Update article via PUT API

Save reference URLs

The pipeline is resilient â€” failures in scraping do not crash the system.

âš ï¸ Error Handling & Design Decisions
External scraping failures are gracefully skipped

LLM output is validated before DB update

Automation runs sequentially to avoid rate limits

ES Modules used for consistency across backend & automation

LLM layer is model-agnostic (Gemini/OpenAI)

ğŸš€ Future Improvements
Frontend dashboard (React)

Scheduled automation (cron jobs)

SEO score comparison

Versioning of articles

Authentication & role-based access

ğŸ‘¤ Author
Palash Bhivgade
Final Year Electronics & Telecommunication Engineering Student
Aspiring Backend / Full Stack Developer

ğŸ“§ Email: pdbhivgade77@gmail.com
ğŸ”— GitHub: https://github.com/palashx7
ğŸ”— LinkedIn: https://www.linkedin.com/in/palash-bhivgade-54089a191

â­ Final Note
This project demonstrates real-world backend engineering, automation design, and AI integration â€” not just CRUD or toy examples.
